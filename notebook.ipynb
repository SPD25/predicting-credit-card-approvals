{"cells":[{"source":"![Credit card being held in hand](credit_card.jpg)\n\nCommercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n### The Data\n\nThe data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value.","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()\n\ncc_apps.info()\n#From first inspection there are no null data points \n\ncc_apps.replace(\"?\", np.nan, inplace=True)\n\nclean_data = cc_apps.copy()\n\n# Convert column index to string to match the key used in the code\nclean_data.columns = clean_data.columns.astype(str)\n\n# Fill NaN values in column \"2\" with the mean of the column\nclean_data[\"2\"] = clean_data[\"2\"].astype(float)  # Ensure the column is of type float\nclean_data[\"2\"].fillna(clean_data[\"2\"].mean(), inplace=True)\n\nclean_data[\"2\"]\n\n# Fixing the loop and conditions\nfor x in clean_data.columns:\n    if clean_data[x].dtype == \"object\":\n        clean_data[x].fillna(clean_data[x].mode()[0], inplace=True)\n    else:\n        clean_data[x].fillna(clean_data[x].mean(), inplace=True)\n        \n# One-hot encoding \nclean_data = pd.get_dummies(clean_data, drop_first=True)\n\n","metadata":{"executionCancelledAt":null,"executionTime":41,"lastExecutedAt":1742589842759,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()\n\ncc_apps.info()\n#From first inspection there are no null data points \n\ncc_apps.replace(\"?\", np.nan, inplace=True)\n\nclean_data = cc_apps.copy()\n\n# Convert column index to string to match the key used in the code\nclean_data.columns = clean_data.columns.astype(str)\n\n# Fill NaN values in column \"2\" with the mean of the column\nclean_data[\"2\"] = clean_data[\"2\"].astype(float)  # Ensure the column is of type float\nclean_data[\"2\"].fillna(clean_data[\"2\"].mean(), inplace=True)\n\nclean_data[\"2\"]\n\n# Fixing the loop and conditions\nfor x in clean_data.columns:\n    if clean_data[x].dtype == \"object\":\n        clean_data[x].fillna(clean_data[x].mode()[0], inplace=True)\n    else:\n        clean_data[x].fillna(clean_data[x].mean(), inplace=True)\n        \n# One-hot encoding \nclean_data = pd.get_dummies(clean_data, drop_first=True)\n\n","outputsMetadata":{"0":{"height":458,"type":"stream"},"1":{"height":550,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"fffa6b24-e18a-49ab-87c8-9951e91e9c17","nodeType":"const"},"quickFilterText":"","sort":{"sortModel":[{"colId":"index","sort":"asc"}]}}}},"lastExecutedByKernel":"bf489e67-7d12-48b1-a274-012a8cf1b7f9"},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 690 entries, 0 to 689\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       690 non-null    object \n 1   1       690 non-null    object \n 2   2       690 non-null    float64\n 3   3       690 non-null    object \n 4   4       690 non-null    object \n 5   5       690 non-null    object \n 6   6       690 non-null    object \n 7   7       690 non-null    float64\n 8   8       690 non-null    object \n 9   9       690 non-null    object \n 10  10      690 non-null    int64  \n 11  11      690 non-null    object \n 12  12      690 non-null    int64  \n 13  13      690 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 75.6+ KB\n"}]},{"source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\n# Assuming clean_data is a DataFrame and the last column is the target variable\nTarget_Variable = clean_data.iloc[:, -1].values\nFeatures = clean_data.iloc[:, :-1].values\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(Features, Target_Variable, test_size=0.30, random_state=42)\n\nscaler = StandardScaler()\n\n# Corrected the method calls for scaling\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nlog = LogisticRegression()\n\n# Corrected the fit method call to include both X_train and y_train\nlog.fit(X_train, y_train)\n\n# Corrected the predict method call to use X_test\ny_pred = log.predict(X_test)\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(conf_matrix)\n\n# Define the parameter grid for GridSearchCV\nparams = {\n    'C': [0.1, 1, 10, 100],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear']\n}\n\ngrid_search = GridSearchCV(estimator=log, param_grid=params, cv=4)\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters found by GridSearchCV\nprint(\"Best parameters found: \", grid_search.best_params_)\nprint(\"Best estimator found: \", grid_search.best_estimator_)\n\nbest_score = grid_search.score(X_test, y_test)\nprint(\"Best score: \", best_score)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":122,"type":"stream"}}},"cell_type":"code","id":"b9238fef-f22a-44bb-9e31-b3cc5277666d","outputs":[{"output_type":"stream","name":"stdout","text":"[[75 22]\n [19 91]]\nBest parameters found:  {'C': 1, 'solver': 'newton-cg'}\nBest estimator found:  LogisticRegression(C=1, solver='newton-cg')\nBest score:  0.8019323671497585\n"}],"execution_count":46}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}